{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1661823223419,"user":{"displayName":"이우원","userId":"05495556713808915653"},"user_tz":-540},"id":"r-q6KMwWtQWa","outputId":"3b3007b0-e088-40a4-97eb-98caa30597ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow_addons\n","  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 15.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.17.1\n"]}],"source":["!pip install tensorflow_addons"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2314,"status":"ok","timestamp":1661823313369,"user":{"displayName":"이우원","userId":"05495556713808915653"},"user_tz":-540},"id":"KJCN058SLgyp","outputId":"aa5bc8fe-33ec-4011-bd21-e2adbe46b427"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","from glob import glob\n","from scipy.io import loadmat\n","import matplotlib.pyplot as plt\n","import random\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/collage/segmentation')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLKzj_yFnttR"},"outputs":[],"source":["rs = 1234\n","# 기존 deeplab v3 + 에서 사용했던 이미지 사이즈를 그대로 사용\n","IMAGE_SIZE = 512\n","# colab pro + 에서 구동할 수 있는 최대의 배치 사이즈 선택\n","BATCH_SIZE = 8\n","# human level 만을 binary로 예측하기때문에 1\n","NUM_CLASSES = 1\n","\n","# 여러 실험을 적용하기 위한 랜덤시드 고정\n","def seed_everything(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","seed_everything(rs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ETU20GzMODpD"},"outputs":[],"source":["DATA_DIR = \"/content/drive/MyDrive/Colab Notebooks/collage/segmentation/data/rawdata/\"\n","\n","# 50000여개의 이미지 + 마스크쌍을 가지고옴\n","all_images = sorted(glob(os.path.join(DATA_DIR, \"humanparsing/val/Images/*\"))+glob(os.path.join(DATA_DIR, \"humanparsing/train/Images/*\"))+glob(os.path.join(DATA_DIR, \"fashion/JPEGImages/*\")))\n","all_masks = sorted(glob(os.path.join(DATA_DIR, \"humanparsing/val/Human/*\"))+glob(os.path.join(DATA_DIR, \"humanparsing/train/Human/*\"))+glob(os.path.join(DATA_DIR, \"fashion/SegmentationClassAug/*\")))\n","\n","# validation size 5000으로 설정 실험 재현읋 위한 random_state 고정\n","from sklearn.model_selection import train_test_split\n","val_size = 5000\n","train_images, val_images, train_masks, val_masks  = train_test_split(all_images, all_masks, test_size=val_size, random_state=rs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2135,"status":"ok","timestamp":1661823937919,"user":{"displayName":"이우원","userId":"05495556713808915653"},"user_tz":-540},"id":"5uhIDgL3Lgys","outputId":"146a7bac-9a46-4a10-e8e8-8ffb8bc6d178"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Dataset: <BatchDataset element_spec=(TensorSpec(shape=(8, 512, 512, 3), dtype=tf.float32, name=None), TensorSpec(shape=(8, 512, 512, 1), dtype=tf.float32, name=None))>\n","Validation Dataset: <BatchDataset element_spec=(TensorSpec(shape=(8, 512, 512, 3), dtype=tf.float32, name=None), TensorSpec(shape=(8, 512, 512, 1), dtype=tf.float32, name=None))>\n"]}],"source":["import tensorflow_addons as tfa\n","\n","def read_image(image_path, mask=False):\n","  '''\n","  하나의 이미지 경로에 대한 이미지를 불러오기위한 함수\n","  image_path : str / 불러올 이미지 경로\n","  mask : bool / 일반이미지의 경우 정규화를 하고 마스크 이미지의 경우 정교화를 하지않음\n","  '''\n","  image = tf.io.read_file(image_path)\n","  image = tf.image.decode_png(image, channels=3)\n","  image.set_shape([None, None, 3])\n","  image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n","  if not mask:\n","      image = image / 255\n","  return image\n","\n","def load_data(image_list, mask_list):\n","  '''\n","  image : str / 불러올 이미지 경로\n","  mask : str / 불러올 마스크 경로\n","  '''\n","  image = read_image(image_list)\n","  mask = read_image(mask_list, mask=True)\n","  return image, mask\n","\n","def random_crop(image, mask, size):\n","  '''\n","  이미지와 마스크 쌍을 인풋하면 각각 같은 크기와 위치를 크롭하여 리턴해주는 함수\n","\n","  image : array / 이미지\n","  mask : array / 마스크\n","  '''\n","  stacked_image = tf.stack([image, mask], axis=0)\n","  cropped_image = tf.image.random_crop(\n","      stacked_image, size=[2, size, size, 3])\n","  img,msk = cropped_image[0], cropped_image[1]\n","  return img,msk\n","\n","def augment(input_image, input_mask):\n","  '''\n","  확률적으로 여러 augmentation을 두 쌍으로 적용하는 함수\n","\n","  input_image : array / 이미지\n","  input_mask : array / 마스크\n","  '''\n","  # 랜덤 크로핑 후 다시 512x512로 리사이즈\n","  if tf.random.uniform(()) > 0.3:\n","      ratio = tf.random.uniform(shape=[],minval=400, maxval=440, dtype=tf.int32)\n","      input_image, input_mask = random_crop(input_image, input_mask, ratio)\n","      input_image = tf.image.resize(input_image, (IMAGE_SIZE, IMAGE_SIZE))\n","      input_mask = tf.image.resize(input_mask, (IMAGE_SIZE, IMAGE_SIZE))\n","  # 이미지에 대해 밝기 조절\n","  input_image = tf.image.random_brightness(input_image, 0.2)\n","  # 색상 대비 조절\n","  saturation_factor = tf.random.uniform((),0,2)\n","  input_image = tf.image.adjust_saturation(input_image, saturation_factor)\n","  # 좌우반전\n","  if tf.random.uniform(()) > 0.5:\n","      input_image = tf.image.flip_left_right(input_image)\n","      input_mask = tf.image.flip_left_right(input_mask)\n","  # 회전\n","  rot_factor = tf.cast(tf.random.uniform(shape=[],minval=-6, maxval=6, dtype=tf.int32), tf.float32)\n","  angle = np.pi/(12*6) *rot_factor\n","  input_image = tfa.image.rotate(input_image, angle)\n","  input_mask = tfa.image.rotate(input_mask, angle)\n","\n","  return input_image, input_mask\n","\n","def cast_mask(input_image, input_mask):\n","  '''\n","  rgb 채널로 구성되어있는 마스킹 정보를 1채널의 정보로 압축하는 함수 인물에 해당하는 픽셀은 1 아닌 픽셀은 0\n","  이미지는 그대로 리턴\n","\n","  input_image : array / 이미지\n","  input_mask : array / 마스크\n","  '''\n","  input_mask = tf.math.reduce_sum(input_mask, axis=2)\n","  input_mask = input_mask[..., tf.newaxis]\n","  input_mask = tf.cast(input_mask>0, dtype=tf.float32)\n","  return input_image,input_mask\n","\n","def data_generator(image_list, mask_list, augument=True):\n","  '''\n","  데이터 제네레이터 정의\n","\n","  image_list : list / 이미지들의 경로가 담긴 리스트\n","  mask_list : list / 마스크들의 경로가 담긴 리스트\n","  '''\n","  dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n","  dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n","  if augument:\n","    dataset = dataset.map(augment)\n","  dataset = dataset.map(cast_mask)\n","  dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","  return dataset\n","\n","# train set에만 agument 적용 \n","train_dataset = data_generator(train_images, train_masks)\n","print(\"Train Dataset:\", train_dataset)\n","\n","val_dataset = data_generator(val_images, val_masks, augument=False)\n","print(\"Validation Dataset:\", val_dataset)"]},{"cell_type":"markdown","source":["![image](https://user-images.githubusercontent.com/102151612/188038684-cd2e6f89-f01c-4cc2-842e-b3aa3340dbf2.png)\n"],"metadata":{"id":"IB3TG6tqyDTH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLGVfa3bLgyz"},"outputs":[],"source":["def conv_block_2times(x, num_filters):\n","  '''\n","  conv -> batch nomalization -> relu 를 두번 반복\n","  x : input\n","  num_filters : 필터의 개수\n","  '''\n","  # 2번반복\n","  for _ in range(2):\n","    x = layers.Conv2D(num_filters, 3, strides=1, padding=\"same\")(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","  return x\n","\n","def encoder_block(x, num_filters):\n","  '''\n","  conv -> batch nomalization -> relu 를 두번 반복 후 피처맵의 크기를 1/2로 줄임\n","  숏컷 구조를 위해 풀링이후의 피처맵과 이전의 피처맵 모두 리턴\n","  x : input\n","  num_filters : 필터의 개수\n","  '''\n","  x = conv_block_2times(x, num_filters)\n","  p = layers.MaxPooling2D((2,2))(x)\n","  return x, p \n","\n","def decoder_block(x, skip, num_filters):\n","  '''\n","  conv -> batch nomalization -> relu 를 두번 반복 후 피처맵의 크기를 1/2로 줄임\n","  숏컷 구조를 위해 풀링이후의 피처맵과 이전의 피처맵 모두 리턴\n","  x : input\n","  skip : concat할 encoder에서의 같은 사이즈의 피처맵\n","  num_filters : 필터의 개수\n","  '''\n","  x = layers.Conv2DTranspose(num_filters, (2,2), strides=2, padding='same')(x)\n","  x = layers.Concatenate(axis=-1)([x, skip])\n","  x = conv_block_2times(x, num_filters)\n","  return x\n","\n","def Unet(image_size, num_classes):\n","  '''\n","  Unet의 구조를 본땀\n","  image_size : 이미지 사이즈\n","  num_classes : 픽셀별 예측할 클래스의 수\n","  '''\n","\n","  inputs = keras.Input(shape=(image_size, image_size, 3))\n","\n","  x1, p1 = encoder_block(inputs, 64)\n","  x2, p2 = encoder_block(p1, 128)\n","  x3, p3 = encoder_block(p2, 256)\n","  x4, p4 = encoder_block(p3, 512)\n","\n","  center = conv_block_2times(p4,1024)\n","\n","  d1 = decoder_block(center, x4, 512)\n","  d2 = decoder_block(d1, x3, 256)\n","  d3 = decoder_block(d2, x2, 128)\n","  d4 = decoder_block(d3, x1, 64)   \n","\n","  model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\", activation='sigmoid')(d4)\n","  # Define the model\n","  return keras.Model(inputs, model_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBKIVmaU4ZFl"},"outputs":[],"source":["model = Unet(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n","loss = keras.losses.BinaryCrossentropy()\n","model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n","    loss=loss,\n","    metrics=[\"accuracy\", tf.keras.metrics.BinaryIoU(),],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCW9gMU7HXjY"},"outputs":[],"source":["from keras import backend\n","from keras.utils import io_utils\n","from tensorflow.python.platform import tf_logging as logging\n","\n","\n","class CustomSaver(keras.callbacks.Callback):\n","  def __init__(self, save_path, save_name, frequency):\n","    self.save_path = save_path\n","    self.save_name = save_name\n","    self.frequency = frequency\n","\n","  def on_epoch_end(self, epoch, logs={}):\n","      if (epoch+1) % self.frequency == 0 :\n","          name = self.save_path + self.save_name + f'{epoch+1:03d}.h5'\n","          self.model.save(name)\n","          io_utils.print_msg(\n","                    f'\\nEpoch {epoch + 1}:'\n","                    f'saving model to {name}')\n","          \n","model_path, model_name = './model_weight/UNet/', 'UNet_epoch_'\n","model_path_best = './model_weight/UNet.h5'\n","csv_name = './model_weight/UNet.csv'\n","\n","# 5 epoch 마다 모델을 저장하도록함 \n","custom_saver = CustomSaver(model_path, model_name ,frequency=5)\n","# 위에서 정의한 loss가 줄지않으면 backbone을 unfreeze 하고 lr을 0.01 -> 0.001로 바꾸어 계속 학습을 이어나감 / lr decay도 계속해서 적용\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, mode='min', min_delta=1e-4, cooldown=0, min_lr=0.0001,)\n","# 가장 좋았던 모델은 따로 저장\n","save_best = tf.keras.callbacks.ModelCheckpoint(model_path_best, monitor='val_loss', verbose=0, save_best_only=True,)\n","# 모델의 로그를 기록\n","write_log = tf.keras.callbacks.CSVLogger(csv_name, separator=',', append=True)\n","\n","cb_list = [custom_saver, reduce_lr, save_best, write_log]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GJoClIVd5Y9j"},"outputs":[],"source":["history = model.fit(train_dataset, validation_data=val_dataset,\n","                    callbacks=cb_list, epochs=100)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/deeplabv3_plus.ipynb","timestamp":1659321316639}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}